#!/bin/bash
set -euo pipefail

# AI Error Analysis Buildkite Plugin - Post-Command Hook
# This hook runs after each command and analyzes errors using AI

# Check if plugin was properly initialized
if [[ "${AI_ERROR_ANALYSIS_INITIALIZED:-false}" != "true" ]]; then
  echo "‚ö†Ô∏è AI Error Analysis plugin not properly initialized, skipping analysis"
  exit 0
fi

# Check if analysis should be skipped
if [[ "${AI_ERROR_ANALYSIS_SKIP:-false}" == "true" ]]; then
  echo "‚ÑπÔ∏è AI Error Analysis skipped for this branch/repository"
  exit 0
fi

# Plugin configuration
PLUGIN_DIR="${AI_ERROR_ANALYSIS_PLUGIN_DIR}"
LOG_PREFIX="${AI_ERROR_ANALYSIS_LOG_PREFIX}"

echo "--- ${LOG_PREFIX} Analyzing command execution"

# Get command exit status
COMMAND_EXIT_STATUS="${BUILDKITE_COMMAND_EXIT_STATUS:-0}"
echo "Command exit status: ${COMMAND_EXIT_STATUS}"

# Configuration
TRIGGER="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_TRIGGER:-auto}"
ASYNC_EXECUTION="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_PERFORMANCE_ASYNC_EXECUTION:-false}"
DEBUG_MODE="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_ADVANCED_DEBUG_MODE:-false}"
DRY_RUN="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_ADVANCED_DRY_RUN:-false}"

# Determine if analysis should be triggered
should_analyze_error() {
  local exit_status="$1"
  local trigger_mode="$2"
  
  case "${trigger_mode}" in
    "always")
      echo "Analysis triggered: always mode"
      return 0
      ;;
    "explicit")
      # Only analyze if explicitly configured to do so
      echo "Analysis skipped: explicit mode requires manual trigger"
      return 1
      ;;
    "auto"|*)
      # Check exit status and conditions
      if [[ "${exit_status}" -eq 0 ]]; then
        echo "Analysis skipped: command succeeded (exit code 0)"
        return 1
      fi
      
      # Check if exit code is in the configured list
      local allowed_exit_codes="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_CONDITIONS_EXIT_STATUS:-[1,2,125,126,127,128,130]}"
      
      python3 -c "
import json
import sys

allowed_codes = json.loads('${allowed_exit_codes}')
current_code = ${exit_status}

if current_code in allowed_codes:
    print(f'Analysis triggered: exit code {current_code} is in allowed list')
    sys.exit(0)
else:
    print(f'Analysis skipped: exit code {current_code} not in allowed list {allowed_codes}')
    sys.exit(1)
"
      return $?
      ;;
  esac
}

# Check if we should analyze this error
if ! should_analyze_error "${COMMAND_EXIT_STATUS}" "${TRIGGER}"; then
  exit 0
fi

# Create temporary directory for analysis
TEMP_DIR=$(mktemp -d)
export AI_ERROR_ANALYSIS_TEMP_DIR="${TEMP_DIR}"

# Cleanup function
cleanup() {
  local exit_code=$?
  if [[ -d "${TEMP_DIR}" ]]; then
    rm -rf "${TEMP_DIR}"
  fi
  exit $exit_code
}
trap cleanup EXIT

# Main analysis function
analyze_error() {
  local start_time=$(date +%s)
  local timeout="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_PERFORMANCE_TIMEOUT:-120}"
  
  echo "--- ${LOG_PREFIX} Starting error analysis"
  
  # Step 1: Detect and categorize the error
  echo "üîç Detecting error patterns..."
  if ! python3 "${PLUGIN_DIR}/lib/error_detector.py" > "${TEMP_DIR}/error_detection.json"; then
    echo "‚ùå Error detection failed"
    return 1
  fi
  
  local error_detected=$(jq -r '.error_detected' "${TEMP_DIR}/error_detection.json")
  if [[ "${error_detected}" != "true" ]]; then
    echo "‚ÑπÔ∏è No specific error patterns detected, performing general analysis"
  fi
  
  # Step 2: Build context for AI analysis
  echo "üìã Building context..."
  if ! python3 "${PLUGIN_DIR}/lib/context_builder.py" > "${TEMP_DIR}/context.json"; then
    echo "‚ùå Context building failed"
    return 1
  fi
  
  # Step 3: Sanitize logs and context
  echo "üßπ Sanitizing logs..."
  if ! python3 "${PLUGIN_DIR}/lib/log_sanitizer.py" \
    "${TEMP_DIR}/context.json" \
    "${TEMP_DIR}/sanitized_context.json"; then
    echo "‚ùå Log sanitization failed"
    return 1
  fi
  
  # Step 4: Check cache for similar errors
  local cache_enabled="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_PERFORMANCE_CACHE_ENABLED:-true}"
  local cached_result=""
  
  if [[ "${cache_enabled}" == "true" ]] && [[ "${DRY_RUN}" != "true" ]]; then
    echo "üíæ Checking cache..."
    cached_result=$(python3 "${PLUGIN_DIR}/lib/cache_manager.py" check "${TEMP_DIR}/sanitized_context.json" || echo "")
    
    if [[ -n "${cached_result}" ]]; then
      echo "‚úÖ Found cached analysis result"
      echo "${cached_result}" > "${TEMP_DIR}/analysis_result.json"
      generate_report
      return 0
    fi
  fi
  
  # Step 5: Call AI provider for analysis
  echo "ü§ñ Calling AI provider..."
  
  if [[ "${DRY_RUN}" == "true" ]]; then
    echo "üß™ Dry run mode - generating mock analysis"
    cat > "${TEMP_DIR}/analysis_result.json" << 'EOF'
{
  "provider": "mock",
  "model": "dry-run",
  "analysis": {
    "root_cause": "This is a mock analysis for dry run mode",
    "suggested_fixes": [
      "Check the dry run configuration",
      "Verify the plugin is working correctly",
      "Review the logs for actual errors"
    ],
    "confidence": 85,
    "error_type": "configuration",
    "severity": "medium"
  },
  "metadata": {
    "analysis_time": "2s",
    "tokens_used": 0,
    "cached": false
  }
}
EOF
  else
    # Timeout wrapper for AI analysis
    timeout "${timeout}" python3 "${PLUGIN_DIR}/lib/ai_providers.py" \
      "${TEMP_DIR}/sanitized_context.json" \
      > "${TEMP_DIR}/analysis_result.json" || {
      local ai_exit_code=$?
      if [[ $ai_exit_code -eq 124 ]]; then
        echo "‚è±Ô∏è AI analysis timed out after ${timeout} seconds"
      else
        echo "‚ùå AI analysis failed with exit code: $ai_exit_code"
      fi
      return 1
    }
  fi
  
  # Step 6: Cache the result (if caching enabled and not dry run)
  if [[ "${cache_enabled}" == "true" ]] && [[ "${DRY_RUN}" != "true" ]]; then
    python3 "${PLUGIN_DIR}/lib/cache_manager.py" store \
      "${TEMP_DIR}/sanitized_context.json" \
      "${TEMP_DIR}/analysis_result.json" || {
      echo "‚ö†Ô∏è Failed to cache analysis result"
    }
  fi
  
  # Step 7: Generate and display report
  generate_report
  
  local end_time=$(date +%s)
  local duration=$((end_time - start_time))
  echo "‚úÖ Analysis completed in ${duration} seconds"
}

# Report generation function
generate_report() {
  echo "üìä Generating report..."
  
  if ! python3 "${PLUGIN_DIR}/lib/report_generator.py" \
    "${TEMP_DIR}/analysis_result.json" \
    "${TEMP_DIR}/context.json" \
    > "${TEMP_DIR}/report.json"; then
    echo "‚ùå Report generation failed"
    return 1
  fi
  
  # Create Buildkite annotation
  create_buildkite_annotation
  
  # Save as artifact if configured
  local save_artifact="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_OUTPUT_SAVE_AS_ARTIFACT:-false}"
  if [[ "${save_artifact}" == "true" ]]; then
    local artifact_path="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_OUTPUT_ARTIFACT_PATH:-ai-analysis-report.json}"
    cp "${TEMP_DIR}/analysis_result.json" "${artifact_path}"
    echo "üíæ Analysis saved as artifact: ${artifact_path}"
  fi
  
  # Debug output
  if [[ "${DEBUG_MODE}" == "true" ]]; then
    echo "--- ${LOG_PREFIX} Debug Information"
    echo "Error detection results:"
    cat "${TEMP_DIR}/error_detection.json" | jq . || echo "Invalid JSON"
    
    echo "Context summary:"
    cat "${TEMP_DIR}/context.json" | jq '.summary // "No summary available"' || echo "N/A"
    
    echo "Analysis result:"
    cat "${TEMP_DIR}/analysis_result.json" | jq . || echo "Invalid JSON"
  fi
}

# Create Buildkite annotation with analysis results
create_buildkite_annotation() {
  local annotation_style="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_OUTPUT_ANNOTATION_STYLE:-error}"
  local annotation_context="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_OUTPUT_ANNOTATION_CONTEXT:-ai-error-analysis}"
  local include_confidence="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_OUTPUT_INCLUDE_CONFIDENCE:-true}"
  
  # Generate HTML annotation
  local html_report=$(python3 "${PLUGIN_DIR}/lib/report_generator.py" html \
    "${TEMP_DIR}/analysis_result.json" \
    "${TEMP_DIR}/context.json" \
    "${include_confidence}")
  
  if [[ -n "${html_report}" ]]; then
    # Create the annotation
    echo "${html_report}" | buildkite-agent annotate \
      --style "${annotation_style}" \
      --context "${annotation_context}"
    
    echo "üìù Analysis annotation created with context: ${annotation_context}"
  else
    echo "‚ùå Failed to generate HTML report for annotation"
    return 1
  fi
}

# Main execution
main() {
  local max_retries="${BUILDKITE_PLUGIN_AI_ERROR_ANALYSIS_ADVANCED_MAX_RETRIES:-3}"
  local retry_count=0
  
  while [[ $retry_count -lt $max_retries ]]; do
    if [[ $retry_count -gt 0 ]]; then
      echo "üîÑ Retry attempt ${retry_count}/${max_retries}"
      sleep $((retry_count * 2))  # Exponential backoff
    fi
    
    if analyze_error; then
      return 0
    else
      ((retry_count++))
      echo "‚ö†Ô∏è Analysis attempt ${retry_count} failed"
    fi
  done
  
  echo "‚ùå Analysis failed after ${max_retries} attempts"
  
  # Create fallback annotation
  buildkite-agent annotate \
    --style "warning" \
    --context "ai-error-analysis-fallback" \
    "‚ö†Ô∏è **AI Error Analysis Failed**<br/>The automated error analysis could not be completed. Please review the logs manually."
  
  # Don't fail the build due to analysis failure
  return 0
}

# Execute based on async configuration
if [[ "${ASYNC_EXECUTION}" == "true" ]]; then
  echo "üöÄ Running analysis in background"
  {
    main
    echo "--- ${LOG_PREFIX} Background analysis completed"
  } &
  echo "‚ÑπÔ∏è Analysis started in background, build will continue"
else
  main
fi